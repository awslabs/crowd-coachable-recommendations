{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51da0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9f137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning==1.9.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4afb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134ce153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e ../crowd-coachable-recommendations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f0a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --ignore-installed -U numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !(cd ../crowd-coachable-recommendations/data/amazon_review_prime_pantry && wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Prime_Pantry.json.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f044843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CCREC_MAX_LENGTH'] = '256'\n",
    "os.environ['CCREC_SIM_TYPE'] = 'dot'\n",
    "os.environ['CCREC_EMBEDDING_TYPE'] = 'mean_pooling'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cb950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f12805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47f18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a5da519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing env defaults\n",
      "using   CCREC_EMBEDDING_TYPE=mean_pooling; options: ['cls', 'mu', 'mean', 'mean_pooling', 'mean_layer_norm']\n",
      "using   CCREC_MAX_LENGTH=256; options: None\n",
      "using   CCREC_SIM_TYPE=dot; options: ['cos', 'dot']\n",
      "setting CCREC_TRAIN_MAIN=bmt_main; options: ['bmt_main', 'bbpr_main']\n",
      "setting CCREC_LEGACY_VAE_BUG=0; options: ['0', '1']\n",
      "setting CCREC_DEBUG_SHAP=0; options: ['0', '1']\n",
      "setting CCREC_TRAINING_PRECISION=32; options: ['32', 'bf16']\n",
      "setting CCREC_BBPR_INV_TEMPERATURE=20; options: None\n",
      "setting CCREC_DISPLAY_LENGTH=250; options: None\n",
      "setting CCREC_INIT_ENV_DONE=1; options: ['0', '1']\n",
      "setting CCREC_NON_BLOCKING=1; options: ['0', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/shared_folder/crowd-coachable-recommendations/src/ccrec/__init__.py:44: UserWarning: dot similarity works best with small inv_temperature\n",
      "  warnings.warn(\"dot similarity works best with small inv_temperature\")\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/utils/_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/maskers/_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/maskers/_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/maskers/_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/shap/explainers/_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run -n ../crowd-coachable-recommendations/scripts/al_oracle_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba28ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4b271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae200428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d572dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = \"nq\"\n",
    "SUFFIX='-contriever-dot'\n",
    "N_STEPS=1\n",
    "MODEL_NAME='facebook/contriever'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0031f761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq_results_oracle_agent_1.0-contriever-dot\n"
     ]
    }
   ],
   "source": [
    "save_root = \"{}_results_oracle_agent_{}{}\".format(DATA_NAME, ACCURACY_LEVEL, SUFFIX)\n",
    "print(save_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a980be",
   "metadata": {},
   "source": [
    "# corpus, queries, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f26b37ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006623506546020508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2681468,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6918d4fac147e8aa97334709e2cecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2681468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus, queries, qrels, *extra = load_data(DATA_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1aa7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(extra):\n",
    "    block_dict, qids_split, item_df = extra\n",
    "    number_of_qid_split_batch = len(qids_split)\n",
    "else:  # standard qids_split creation, though we used previous splits for stability\n",
    "    block_dict = None\n",
    "    number_of_qid_split_batch = 4\n",
    "    qids_split = np.array_split(\n",
    "        np.random.RandomState(42).permutation(list(queries.keys())),\n",
    "        number_of_qid_split_batch\n",
    "    )\n",
    "    qids_split = [s.tolist() for s in qids_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b977cd8",
   "metadata": {},
   "source": [
    "# bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bm_25 import BM25\n",
    "import tqdm\n",
    "\n",
    "bm25_agent = BM25(b=0.75, k1=1.2).fit(corpus.values())\n",
    "corpus_keys = list(corpus.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d379c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3452/3452 [21:19<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_profile = {}\n",
    "for qid in tqdm.tqdm(queries.keys()):\n",
    "    scores = bm25_agent.transform(queries[qid])\n",
    "    if block_dict is not None:\n",
    "        scores[\n",
    "            item_df.index.get_indexer(block_dict[qid])\n",
    "        ] = -1e6\n",
    "    top100 = torch.as_tensor(scores).topk(100).indices.numpy()\n",
    "    bm25_profile[qid] = {\n",
    "        corpus_keys[a]: scores[a]\n",
    "        for a in top100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd4c9927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR@1': 0.1489, 'MRR@5': 0.23039, 'MRR@10': 0.24446, 'MRR@100': 0.25604}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = EvaluateRetrieval(None)\n",
    "evaluator.evaluate_custom(\n",
    "    qrels, bm25_profile, [1, 5, 10, 100], metric=\"mrr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6247b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_path = f\"{save_root}/data_iteration_0/ranking_profile_bm25.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beeb4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {os.path.dirname(bm25_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e63e6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bm25_profile, bm25_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c7417",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68064d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy level: 1.0\n",
      "Total number of iterations: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy level:\", ACCURACY_LEVEL)\n",
    "print(\"Total number of iterations:\", number_of_qid_split_batch * N_STEPS)\n",
    "\n",
    "ranking_profile_bm25 = torch.load(bm25_path)\n",
    "\n",
    "# Load initial model\n",
    "model_init_dir = None\n",
    "model = _BertBPR(None, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c0c017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1d3ef23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate ranking profile at step: 0\n",
      "caching replicas\n",
      "Processed 2048 | 3452 t=2.5s / 4.3s\n",
      "Processed total 3452 t=3.2s\n",
      "Processed 2048 | 2681468 t=0.8s / 1024.5s\n",
      "Processed 4096 | 2681468 t=1.6s / 1027.8s\n",
      "Processed 8192 | 2681468 t=3.1s / 1023.2s\n",
      "Processed 16384 | 2681468 t=6.3s / 1033.3s\n",
      "Processed 32768 | 2681468 t=13.3s / 1087.5s\n",
      "Processed 65536 | 2681468 t=27.9s / 1143.1s\n",
      "Processed 131072 | 2681468 t=56.8s / 1162.8s\n",
      "Processed 262144 | 2681468 t=114.9s / 1175.6s\n",
      "Processed 524288 | 2681468 t=231.0s / 1181.6s\n",
      "Processed 1048576 | 2681468 t=463.3s / 1184.7s\n",
      "Processed 2097152 | 2681468 t=930.7s / 1190.0s\n",
      "Processed total 2681468 t=1205.2s\n",
      "MRR@1 : 0.14455\n",
      "MRR@5 : 0.22817\n",
      "MRR@10 : 0.24384\n",
      "MRR@100 : 0.25769\n",
      "MRR@1 : 0.14455\n",
      "MRR@5 : 0.22817\n",
      "MRR@10 : 0.24384\n",
      "MRR@100 : 0.25769\n",
      "Generate training data at step: 0\n",
      "Training model at step: 0\n",
      "Number of training data: 274\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(274, 1357) with 274 target events and 1358 prior scores created!\n",
      "multiple_nrl alpha=1.0\n",
      "BertMT logs at logs/BertMT/version_25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `Trainer(gpus=4)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=4)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_nrl alpha=1.0\n",
      "masked False\n",
      "ct_num_batches 45.233333333333334 ft_num_batches 9.133333333333333 ct_cycles 1 ft_cycles 4.952554744525548\n",
      "train_set size 274\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007637977600097656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 1357,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1357 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,5,6,7]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | item_tower | NaiveItemTower | 109 M \n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_25/checkpoints\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006483554840087891,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One of given dataloaders is None and it will be skipped.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060083866119384766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a281d1ea4d244d8187cecfca2ef94381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "`ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['loss', 'loss_step', 'loss_epoch', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "No checkpoints found!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.load_state_dict(torch.load(...), strict=False)\n",
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_25/checkpoints/state-dict.pth\n",
      "fit time 275.7s\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(274, 1357) with 274 target events and 1358 prior scores created!\n",
      "masked False\n",
      "ct_num_batches 45.233333333333334 ft_num_batches 9.133333333333333 ct_cycles 1 ft_cycles 4.952554744525548\n",
      "Processing 0 | 2\n",
      "transform time 2.2s\n",
      "_assign_topk time 0.0s\n",
      "evaluate_assigned time 0.0s\n",
      "Generate ranking profile at step: 1\n",
      "caching replicas\n",
      "Processed 2048 | 3452 t=0.3s / 0.6s\n",
      "Processed total 3452 t=0.5s\n",
      "Processed 2048 | 2681468 t=0.8s / 1028.9s\n",
      "Processed 4096 | 2681468 t=1.6s / 1031.9s\n",
      "Processed 8192 | 2681468 t=3.1s / 1027.8s\n",
      "Processed 16384 | 2681468 t=6.9s / 1121.3s\n",
      "Processed 32768 | 2681468 t=13.3s / 1088.3s\n",
      "Processed 65536 | 2681468 t=27.8s / 1138.3s\n",
      "Processed 131072 | 2681468 t=56.9s / 1163.4s\n",
      "Processed 262144 | 2681468 t=115.0s / 1176.0s\n",
      "Processed 524288 | 2681468 t=231.1s / 1182.2s\n",
      "Processed 1048576 | 2681468 t=463.5s / 1185.4s\n",
      "Processed 2097152 | 2681468 t=928.3s / 1186.9s\n",
      "Processed total 2681468 t=1189.2s\n",
      "MRR@1 : 0.29838\n",
      "MRR@5 : 0.39001\n",
      "MRR@10 : 0.40534\n",
      "MRR@100 : 0.41558\n",
      "MRR@1 : 0.29838\n",
      "MRR@5 : 0.39001\n",
      "MRR@10 : 0.40534\n",
      "MRR@100 : 0.41558\n",
      "Generate training data at step: 1\n",
      "Training model at step: 1\n",
      "Number of training data: 685\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(685, 3392) with 685 target events and 3395 prior scores created!\n",
      "multiple_nrl alpha=1.0\n",
      "BertMT logs at logs/BertMT/version_30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `Trainer(gpus=4)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=4)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_nrl alpha=1.0\n",
      "masked False\n",
      "ct_num_batches 113.06666666666666 ft_num_batches 22.833333333333332 ct_cycles 1 ft_cycles 4.951824817518248\n",
      "train_set size 685\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005225658416748047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 3392,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,5,6,7]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | item_tower | NaiveItemTower | 109 M \n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_30/checkpoints\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005771636962890625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One of given dataloaders is None and it will be skipped.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005277156829833984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc391b643d14e2c8c2153ade63a2cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 25. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "No checkpoints found!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.load_state_dict(torch.load(...), strict=False)\n",
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_30/checkpoints/state-dict.pth\n",
      "fit time 711.0s\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(685, 3392) with 685 target events and 3395 prior scores created!\n",
      "masked False\n",
      "ct_num_batches 113.06666666666666 ft_num_batches 22.833333333333332 ct_cycles 1 ft_cycles 4.951824817518248\n",
      "Processing 0 | 4\n",
      "transform time 5.0s\n",
      "_assign_topk time 0.0s\n",
      "evaluate_assigned time 0.0s\n",
      "Generate ranking profile at step: 2\n",
      "caching replicas\n",
      "Processed 2048 | 3452 t=0.3s / 0.4s\n",
      "Processed total 3452 t=0.4s\n",
      "Processed 2048 | 2681468 t=0.8s / 1001.6s\n",
      "Processed 4096 | 2681468 t=1.5s / 1009.3s\n",
      "Processed 8192 | 2681468 t=3.1s / 1005.1s\n",
      "Processed 16384 | 2681468 t=6.5s / 1071.0s\n",
      "Processed 32768 | 2681468 t=13.4s / 1099.6s\n",
      "Processed 65536 | 2681468 t=27.8s / 1139.1s\n",
      "Processed 131072 | 2681468 t=56.9s / 1164.3s\n",
      "Processed 262144 | 2681468 t=115.2s / 1178.2s\n",
      "Processed 524288 | 2681468 t=231.4s / 1183.6s\n",
      "Processed 1048576 | 2681468 t=463.9s / 1186.3s\n",
      "Processed 2097152 | 2681468 t=929.0s / 1187.8s\n",
      "Processed total 2681468 t=1190.1s\n",
      "MRR@1 : 0.36935\n",
      "MRR@5 : 0.44443\n",
      "MRR@10 : 0.45795\n",
      "MRR@100 : 0.46755\n",
      "MRR@1 : 0.36935\n",
      "MRR@5 : 0.44443\n",
      "MRR@10 : 0.45795\n",
      "MRR@100 : 0.46755\n",
      "Generate training data at step: 2\n",
      "Training model at step: 2\n",
      "Number of training data: 1066\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.5s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(1066, 5271) with 1066 target events and 5281 prior scores created!\n",
      "multiple_nrl alpha=1.0\n",
      "BertMT logs at logs/BertMT/version_31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `Trainer(gpus=4)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=4)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_nrl alpha=1.0\n",
      "masked False\n",
      "ct_num_batches 175.7 ft_num_batches 35.53333333333333 ct_cycles 1 ft_cycles 4.9446529080675425\n",
      "train_set size 1066\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005379915237426758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 5271,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,5,6,7]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | item_tower | NaiveItemTower | 109 M \n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_31/checkpoints\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005673885345458984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One of given dataloaders is None and it will be skipped.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0052204132080078125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6393b7326548e2a5b81512d4952b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "No checkpoints found!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.load_state_dict(torch.load(...), strict=False)\n",
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_31/checkpoints/state-dict.pth\n",
      "fit time 1079.8s\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(1066, 5271) with 1066 target events and 5281 prior scores created!\n",
      "masked False\n",
      "ct_num_batches 175.7 ft_num_batches 35.53333333333333 ct_cycles 1 ft_cycles 4.9446529080675425\n",
      "Processing 0 | 6\n",
      "transform time 7.3s\n",
      "_assign_topk time 0.0s\n",
      "evaluate_assigned time 0.0s\n",
      "Generate ranking profile at step: 3\n",
      "caching replicas\n",
      "Processed 2048 | 3452 t=0.3s / 0.6s\n",
      "Processed total 3452 t=0.4s\n",
      "Processed 2048 | 2681468 t=0.8s / 1005.7s\n",
      "Processed 4096 | 2681468 t=1.5s / 1009.2s\n",
      "Processed 8192 | 2681468 t=3.1s / 999.7s\n",
      "Processed 16384 | 2681468 t=6.2s / 1015.8s\n",
      "Processed 32768 | 2681468 t=13.3s / 1087.5s\n",
      "Processed 65536 | 2681468 t=27.8s / 1138.2s\n",
      "Processed 131072 | 2681468 t=56.9s / 1163.5s\n",
      "Processed 262144 | 2681468 t=115.2s / 1178.3s\n",
      "Processed 524288 | 2681468 t=231.4s / 1183.6s\n",
      "Processed 1048576 | 2681468 t=463.7s / 1185.8s\n",
      "Processed 2097152 | 2681468 t=928.6s / 1187.3s\n",
      "Processed total 2681468 t=1189.6s\n",
      "MRR@1 : 0.41425\n",
      "MRR@5 : 0.4832\n",
      "MRR@10 : 0.49471\n",
      "MRR@100 : 0.50381\n",
      "MRR@1 : 0.41425\n",
      "MRR@5 : 0.4832\n",
      "MRR@10 : 0.49471\n",
      "MRR@100 : 0.50381\n",
      "Generate training data at step: 3\n",
      "Training model at step: 3\n",
      "Number of training data: 1498\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(1498, 7401) with 1498 target events and 7427 prior scores created!\n",
      "multiple_nrl alpha=1.0\n",
      "BertMT logs at logs/BertMT/version_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `Trainer(gpus=4)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=4)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A10G') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiple_nrl alpha=1.0\n",
      "masked False\n",
      "ct_num_batches 246.7 ft_num_batches 49.93333333333333 ct_cycles 1 ft_cycles 4.940587449933244\n",
      "train_set size 1498\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0052585601806640625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 7401,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [4,5,6,7]\n",
      "\n",
      "  | Name       | Type           | Params\n",
      "----------------------------------------------\n",
      "0 | item_tower | NaiveItemTower | 109 M \n",
      "----------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_32/checkpoints\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0057375431060791016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One of given dataloaders is None and it will be skipped.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005127906799316406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39aad2af37134a9bac2fc4c45bcfa689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 28. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "No checkpoints found!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.load_state_dict(torch.load(...), strict=False)\n",
      "/home/ec2-user/efs/shared_folder/notebook_prime_pantry/lightning_logs/version_32/checkpoints/state-dict.pth\n",
      "fit time 1522.1s\n",
      "timing checking for repeated user-item events done . time 0.0s\n",
      "timing inferring training events done . time 0.0s\n",
      "timing creating target_csr timing joining testing events by multi-indexed requests done . time 0.0s\n",
      "done . time 0.0s\n",
      "timing creating prior_score done . time 0.0s\n",
      "timing creating reranking tasks by adding prior_score from target candidates done . time 0.0s\n",
      "Dataset(1498, 7401) with 1498 target events and 7427 prior scores created!\n",
      "masked False\n",
      "ct_num_batches 246.7 ft_num_batches 49.93333333333333 ct_cycles 1 ft_cycles 4.940587449933244\n",
      "Processing 0 | 8\n",
      "transform time 10.1s\n",
      "_assign_topk time 0.1s\n",
      "evaluate_assigned time 0.0s\n",
      "Generate ranking profile at step: 4\n",
      "caching replicas\n",
      "Processed 2048 | 3452 t=0.2s / 0.4s\n",
      "Processed total 3452 t=0.4s\n",
      "Processed 2048 | 2681468 t=0.8s / 1052.3s\n",
      "Processed 4096 | 2681468 t=1.6s / 1035.4s\n",
      "Processed 8192 | 2681468 t=3.1s / 1017.2s\n",
      "Processed 16384 | 2681468 t=6.2s / 1021.2s\n",
      "Processed 32768 | 2681468 t=13.6s / 1109.1s\n",
      "Processed 65536 | 2681468 t=28.2s / 1154.5s\n",
      "Processed 131072 | 2681468 t=57.3s / 1171.7s\n",
      "Processed 262144 | 2681468 t=115.2s / 1178.5s\n",
      "Processed 524288 | 2681468 t=231.4s / 1183.6s\n",
      "Processed 1048576 | 2681468 t=463.7s / 1185.8s\n",
      "Processed 2097152 | 2681468 t=928.6s / 1187.3s\n",
      "Processed total 2681468 t=1189.6s\n",
      "MRR@1 : 0.47827\n",
      "MRR@5 : 0.53532\n",
      "MRR@10 : 0.54807\n",
      "MRR@100 : 0.55574\n",
      "MRR@1 : 0.47827\n",
      "MRR@5 : 0.53532\n",
      "MRR@10 : 0.54807\n",
      "MRR@100 : 0.55574\n"
     ]
    }
   ],
   "source": [
    "for step in range(N_STEPS * number_of_qid_split_batch + 1):\n",
    "    previous_working_dir = f\"{save_root}/data_iteration_{step-1}\"\n",
    "    current_working_dir = f\"{save_root}/data_iteration_{step}\"\n",
    "\n",
    "    if not os.path.exists(current_working_dir):\n",
    "        os.makedirs(current_working_dir)\n",
    "\n",
    "    # generate ranking profile\n",
    "    print(\"Generate ranking profile at step:\", step)\n",
    "    save_path = os.path.join(current_working_dir, \"ranking_profile.pt\")\n",
    "    if os.path.isfile(save_path):\n",
    "        ranking_profile = torch.load(save_path)\n",
    "    else:\n",
    "        with autocast():\n",
    "            ranking_profile = generate_ranking_profile(\n",
    "                model, MODEL_NAME, corpus, queries, qrels, save_path, block_dict)\n",
    "\n",
    "    evaluator = EvaluateRetrieval(None)\n",
    "    mrr = evaluator.evaluate_custom(\n",
    "        qrels, ranking_profile, [1, 5, 10, 100], metric=\"mrr\"\n",
    "    )\n",
    "    for name, value in mrr.items():\n",
    "        print(\"{}\".format(name), \":\", value)\n",
    "\n",
    "    if step >= N_STEPS * number_of_qid_split_batch:\n",
    "        break  # moved here after running\n",
    "\n",
    "    # generate training data\n",
    "    print(\"Generate training data at step:\", step)\n",
    "\n",
    "    train_data = generate_train_data(\n",
    "        qids_split[step % number_of_qid_split_batch],\n",
    "        qrels,\n",
    "        ranking_profile,\n",
    "        ranking_profile_bm25,\n",
    "        list(corpus.keys()),\n",
    "        step,\n",
    "    )\n",
    "\n",
    "    if step > 0:\n",
    "        train_data_prev_dir = os.path.join(\n",
    "            previous_working_dir, \"training_data.pt\"\n",
    "        )\n",
    "        train_data_prev = torch.load(train_data_prev_dir)\n",
    "        train_data = combine_train_data(train_data_prev, train_data)\n",
    "\n",
    "    train_data_dir = os.path.join(current_working_dir, \"training_data.pt\")\n",
    "    torch.save(train_data, train_data_dir)\n",
    "    \n",
    "    # train model\n",
    "    print(\"Training model at step:\", step)\n",
    "    print(\"Number of training data:\", len(train_data))\n",
    "    model = training(\n",
    "        train_data,\n",
    "        NUM_EPOCHS,\n",
    "        model_checkpoint=model_init_dir,\n",
    "        corpus=corpus,\n",
    "        queries=queries,\n",
    "        model_selection=MODEL_NAME,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        model.model.item_tower.state_dict(),\n",
    "        f'{current_working_dir}/state-dict.pth',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0a643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07547e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c0c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49c63b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_mrr():\n",
    "    for step in range(number_of_qid_split_batch+1):\n",
    "        current_working_dir = f\"{save_root}/data_iteration_{step}\"\n",
    "        ranking_profile = torch.load(current_working_dir + '/ranking_profile.pt')\n",
    "        evaluator = EvaluateRetrieval(None)\n",
    "        mrr = evaluator.evaluate_custom(\n",
    "            qrels, ranking_profile, [1, 5, 10, 100], metric=\"mrr\"\n",
    "        )\n",
    "        for name, value in mrr.items():\n",
    "            print(\"{}\".format(name), \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbe368a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 : 0.14455\n",
      "MRR@5 : 0.22817\n",
      "MRR@10 : 0.24384\n",
      "MRR@100 : 0.25769\n",
      "MRR@1 : 0.29838\n",
      "MRR@5 : 0.39001\n",
      "MRR@10 : 0.40534\n",
      "MRR@100 : 0.41558\n",
      "MRR@1 : 0.36935\n",
      "MRR@5 : 0.44443\n",
      "MRR@10 : 0.45795\n",
      "MRR@100 : 0.46755\n",
      "MRR@1 : 0.41425\n",
      "MRR@5 : 0.4832\n",
      "MRR@10 : 0.49471\n",
      "MRR@100 : 0.50381\n",
      "MRR@1 : 0.47827\n",
      "MRR@5 : 0.53532\n",
      "MRR@10 : 0.54807\n",
      "MRR@100 : 0.55574\n"
     ]
    }
   ],
   "source": [
    "print_all_mrr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce2a0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gen_mrr():\n",
    "    for step in range(number_of_qid_split_batch+1):\n",
    "        current_working_dir = f\"{save_root}/data_iteration_{step}\"\n",
    "        ranking_profile = torch.load(current_working_dir + '/ranking_profile.pt')\n",
    "        evaluator = EvaluateRetrieval(None)\n",
    "        mrr = evaluator.evaluate_custom(\n",
    "            {k:v for k,v in qrels.items() if k in qids_split[-1]},\n",
    "            {k:v for k,v in ranking_profile.items() if k in qids_split[-1]},\n",
    "            [1, 5, 10, 100], metric=\"mrr\"\n",
    "        )\n",
    "        for name, value in mrr.items():\n",
    "            print(\"{}\".format(name), \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "067cfccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR@1 : 0.13905\n",
      "MRR@5 : 0.22578\n",
      "MRR@10 : 0.24181\n",
      "MRR@100 : 0.25584\n",
      "MRR@1 : 0.26999\n",
      "MRR@5 : 0.37466\n",
      "MRR@10 : 0.39031\n",
      "MRR@100 : 0.40062\n",
      "MRR@1 : 0.30012\n",
      "MRR@5 : 0.3995\n",
      "MRR@10 : 0.41412\n",
      "MRR@100 : 0.4241\n",
      "MRR@1 : 0.30591\n",
      "MRR@5 : 0.42016\n",
      "MRR@10 : 0.43279\n",
      "MRR@100 : 0.4423\n",
      "MRR@1 : 0.5168\n",
      "MRR@5 : 0.56419\n",
      "MRR@10 : 0.57684\n",
      "MRR@100 : 0.58492\n"
     ]
    }
   ],
   "source": [
    "print_gen_mrr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fc0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1103e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
